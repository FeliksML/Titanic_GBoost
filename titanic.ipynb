{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Titanic\n",
    "## Predict survival on the Titanic using Gradient Boost\n"
   ],
   "id": "2e63d4d6e38bcb17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load data into Pandas DataFrame",
   "id": "a78c9950875aee08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read into DataFrames\n",
    "train = pd.read_csv(\"titanic/train.csv\")\n",
    "test = pd.read_csv(\"titanic/test.csv\")\n",
    "\n",
    "# At first lets check the train DataFrame\n",
    "test"
   ],
   "id": "54f37c105415d0a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check for `null` values",
   "id": "a00de73e24516d85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train.isnull().sum()",
   "id": "8e9c469399aa862d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Null values found in `Age: 177`, `Cabin: 687`, `Embarked: 2`",
   "id": "6463f7af2e6fd81"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check data types",
   "id": "dd43453f18c6702"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train.info()",
   "id": "cd4bced10236f509",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check distribution of `Age` before imputing null values",
   "id": "a83f8bd3ba775f3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot a histogram with Age variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train[\"Age\"], bins=30, kde=True)\n",
    "plt.title(\"Age distribution\")\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "id": "2dbb14d302651520",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Impute null values for `Age` variable where distribution is right-skewed\n",
    "\n",
    "### When we get right-skewed distribution of a numerical variable the better practice is use `median` value for imputing missing values. We can't use `mean` value because it will not represent the central tendency of the distribution, because the mean value will come from the right area of distribution.\n",
    "### But for the this time I will use MICE to impute those values because this technique especially good for non-normally distributed  or skewed data."
   ],
   "id": "d34b71992be3ee0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import miceforest as mf\n",
    "\n",
    "# Prepare Sex columns\n",
    "mapping = {'male': 1, 'female': 0}\n",
    "train[\"Sex\"] = train[\"Sex\"].map(mapping)\n",
    "test[\"Sex\"] = test[\"Sex\"].map(mapping)\n",
    "\n",
    "# Create kernel for train dataset \n",
    "kernel = mf.ImputationKernel(data=train[[\"Age\", \"Survived\", \"Fare\", \"Sex\"]], datasets=1, save_all_iterations=True)\n",
    "\n",
    "# Launch the MICE imputation for train dataset\n",
    "kernel.mice(10)\n",
    "\n",
    "# Kernel for test dataset\n",
    "kernel_test = mf.ImputationKernel(data=test[[\"Age\", \"Fare\", \"Sex\"]], datasets=1, save_all_iterations=True)\n",
    "\n",
    "# Launch the kernel_test\n",
    "kernel_test.mice(10)\n",
    "\n",
    "# Get imputed data for test and train datasets\n",
    "imputed_data_train = kernel.complete_data(0)\n",
    "imputed_data_test = kernel_test.complete_data(0)\n",
    "\n",
    "# Replace old columns with new columns\n",
    "train[[\"Age\", \"Survived\", \"Fare\", \"Sex\"]] = imputed_data_train\n",
    "test[[\"Age\", \"Fare\", \"Sex\"]] = imputed_data_test\n",
    "\n",
    "# Create subplots \n",
    "_, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 12))\n",
    "\n",
    "# Plot for train dataset\n",
    "sns.histplot(train['Age'], bins=30, kde=True, ax=axes[0])\n",
    "axes[0].set_title(\"Train Dataset. Imputed Age\")\n",
    "axes[0].set_xlabel(\"Age\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Plot for test dataset\n",
    "sns.histplot(test['Age'], bins=30, kde=True, ax=axes[1])\n",
    "axes[1].set_title(\"Test Dataset. Imputed Age\")\n",
    "axes[1].set_xlabel(\"Age\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "1c5fb089c1e06f9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train.isnull().sum()",
   "id": "3d6668ad2043eed8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Cabin \n",
    "### Which passengers had assigned cabins and which did not?  "
   ],
   "id": "204f52009349d25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train['Cabin'].fillna('Missing', inplace=True)\n",
    "cabin = train['Cabin'].apply(lambda x: 'Has Cabin' if x != 'Missing' else 'Missing')\n",
    "contingency_table = pd.crosstab(train['Pclass'], cabin)\n",
    "_, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 6))\n",
    "\n",
    "# Stacked Bar Plot\n",
    "contingency_table.plot(kind='bar', stacked=True, colormap='viridis', ax=axes[0])\n",
    "axes[0].set_title('Stacked Bar Plot of Pclass vs Cabin')\n",
    "axes[0].set_xlabel('Pclass')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Grouped Bar Plot\n",
    "contingency_table.plot(kind='bar', colormap='viridis', ax=axes[1])\n",
    "axes[1].set_title('Grouped Bar Plot of Pclass vs Cabin')\n",
    "axes[1].set_xlabel('Pclass')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "8f502d95203715b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# There is a lot of missing values in Third and Second class tickets so it's better to just drop the `Cabin` column.\n",
   "id": "1569678dbff04079"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train.drop(columns=['Cabin'], inplace=True)\n",
    "test.drop(columns=['Cabin'], inplace=True)"
   ],
   "id": "e86ee56beef281d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Impute `Embarked` variable with mod value",
   "id": "9498b26c6e756a53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Impute with mod value\n",
    "train['Embarked'].fillna(train['Embarked'].mode()[0], inplace=True)\n",
    "test['Embarked'].fillna(test['Embarked'].mode()[0], inplace=True)"
   ],
   "id": "e21109a9b48bdb94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Drop `PassengerId`",
   "id": "2048be17b6374b35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train.drop(columns=['PassengerId'], inplace=True)\n",
    "#test.drop(columns=['PassengerId'], inplace=True)"
   ],
   "id": "7553de547de858be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenize `Name` and `Ticket`\n",
   "id": "36c9646b9c0474eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tokenize(df: pd.DataFrame):\n",
    "    \"\"\"Tokenizing the columns\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    def name(x: str):\n",
    "        \"\"\"Splitting the string using space character then removing non-alphabetic characters\"\"\"\n",
    "        return \"\".join([v.strip(\",()[].\\\"'\") for v in x.split(\" \")])\n",
    "\n",
    "    def ticket_number(x: str):\n",
    "        \"\"\"Splitting the string by space and then taking the last element\"\"\"\n",
    "        return x.split(\" \")[-1]\n",
    "\n",
    "    def ticket_item(x: str):\n",
    "        \"\"\"Splitting string then if length 2D array is 1 it means there's no item otherwise we take return all items\"\"\"\n",
    "        item = x.split(\" \")\n",
    "        if len(item) == 1:\n",
    "            return \"NONE\"\n",
    "        return \" \".join(item[0:-1])\n",
    "\n",
    "    df[\"Name\"] = df[\"Name\"].apply(name)\n",
    "    df[\"Ticket_number\"] = df[\"Ticket\"].apply(ticket_number)\n",
    "    df[\"Ticket_item\"] = df[\"Ticket\"].apply(ticket_item)\n",
    "    return df\n",
    "\n",
    "\n",
    "preprocessed_train = tokenize(train)\n",
    "preprocessed_test = tokenize(test)"
   ],
   "id": "38ffc2e48ca37304",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create input features list",
   "id": "2418009f8d339848"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_features = list(preprocessed_train.columns)\n",
    "input_features.remove(\"Ticket\")\n",
    "input_features.remove(\"Survived\")\n",
    "input_features"
   ],
   "id": "9ce6ed642888ec35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Convert Pandas DataFrame to TensorFlow Dataset\n",
   "id": "4e048a8afc1ad36e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Configure TensorFlow to use the Apple Metal plugin before any other operations\n",
    "print(tf.config.list_physical_devices())\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "\n",
    "def tokenize_name(features, labels=None):\n",
    "    features[\"Name\"] = tf.strings.split(features[\"Name\"])\n",
    "    return features, labels"
   ],
   "id": "b15fb9078d10b172",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Convert Pandas DataFrame into tensorflow dataset",
   "id": "62ebf59c5ee9bc24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tf_train = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_train, label=\"Survived\").map(tokenize_name)\n",
    "tf_test = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_test).map(tokenize_name)"
   ],
   "id": "170f2842f05b5c8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train Gradient Boost model with default parameters",
   "id": "d629f991c71e0321"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = tfdf.keras.GradientBoostedTreesModel(\n",
    "    verbose=0,\n",
    "    features=[tfdf.keras.FeatureUsage(n) for n in input_features],\n",
    "    exclude_non_specified_features=True,\n",
    "    random_seed=9\n",
    ")\n",
    "model.fit(tf_train)\n",
    "\n",
    "self_eval = model.make_inspector().evaluation()\n",
    "print(f\"Accuracy={self_eval.accuracy}, Loss={self_eval.loss}\")"
   ],
   "id": "9263ad6eb668873",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.summary()",
   "id": "ff1235d76aa99659",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training a model with hyperparameter tunning ",
   "id": "bdcff84c4ca4f26e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# Create a Random Search tuner with 50 trials.\n",
    "tuner = tfdf.tuner.RandomSearch(num_trials=100)\n",
    "\n",
    "# Define the search space.\n",
    "#\n",
    "# Adding more parameters generaly improve the quality of the model, but make\n",
    "# the tuning last longer.\n",
    "\n",
    "tuner.choice(\"min_examples\", [2, 5, 7, 10])\n",
    "tuner.choice(\"categorical_algorithm\", [\"CART\", \"RANDOM\"])\n",
    "\n",
    "# Some hyper-parameters are only valid for specific values of other\n",
    "# hyper-parameters. For example, the \"max_depth\" parameter is mostly useful when\n",
    "# \"growing_strategy=LOCAL\" while \"max_num_nodes\" is better suited when\n",
    "# \"growing_strategy=BEST_FIRST_GLOBAL\".\n",
    "\n",
    "local_search_space = tuner.choice(\"growing_strategy\", [\"LOCAL\"])\n",
    "local_search_space.choice(\"max_depth\", [3, 4, 5, 6, 8])\n",
    "\n",
    "# merge=True indicates that the parameter (here \"growing_strategy\") is already\n",
    "# defined, and that new values are added to it.\n",
    "global_search_space = tuner.choice(\"growing_strategy\", [\"BEST_FIRST_GLOBAL\"], merge=True)\n",
    "global_search_space.choice(\"max_num_nodes\", [16, 32, 64, 128, 256])\n",
    "\n",
    "#tuner.choice(\"use_hessian_gain\", [True, False])\n",
    "tuner.choice(\"shrinkage\", [0.02, 0.05, 0.10, 0.15])\n",
    "tuner.choice(\"num_candidate_attributes_ratio\", [0.2, 0.5, 0.9, 1.0])\n",
    "\n",
    "# Uncomment some (or all) of the following hyper-parameters to increase the\n",
    "# quality of the search. The number of trial should be increased accordingly.\n",
    "\n",
    "tuner.choice(\"split_axis\", [\"AXIS_ALIGNED\"])\n",
    "oblique_space = tuner.choice(\"split_axis\", [\"SPARSE_OBLIQUE\"], merge=True)\n",
    "oblique_space.choice(\"sparse_oblique_normalization\",\n",
    "                     [\"NONE\", \"STANDARD_DEVIATION\", \"MIN_MAX\"])\n",
    "oblique_space.choice(\"sparse_oblique_weights\", [\"BINARY\", \"CONTINUOUS\"])\n",
    "oblique_space.choice(\"sparse_oblique_num_projections_exponent\", [1.0, 1.5])\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "# train_df, val_df = train_test_split(preprocessed_train, test_size=0.2, random_state=42)\n",
    "# \n",
    "# # Create TensorFlow datasets\n",
    "# tf_train = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"Survived\").map(tokenize_name)\n",
    "# tf_val = tfdf.keras.pd_dataframe_to_tf_dataset(val_df, label=\"Survived\").map(tokenize_name)\n",
    "with tf.device('/GPU:0'):\n",
    "    for seed in range(100):\n",
    "        print(f\"SEED: {seed}\\n\")\n",
    "        tuned_model = tfdf.keras.GradientBoostedTreesModel(\n",
    "            verbose=0,\n",
    "            features=[tfdf.keras.FeatureUsage(n) for n in input_features],\n",
    "            exclude_non_specified_features=True,\n",
    "            random_seed=seed,\n",
    "            tuner=tuner,\n",
    "            task=tfdf.keras.Task.CLASSIFICATION,\n",
    "            sampling_method=\"GOSS\", goss_alpha=0.15, goss_beta=0.15\n",
    "        )\n",
    "        tuned_model.fit(tf_train, verbose=0)\n",
    "\n",
    "        tuned_self_evaluation = tuned_model.make_inspector().evaluation()\n",
    "        print(f\"Accuracy: {tuned_self_evaluation.accuracy} Loss:{tuned_self_evaluation.loss}\")\n",
    "\n",
    "        # Update the best model if this one is better\n",
    "        if tuned_self_evaluation.accuracy > best_accuracy or (tuned_self_evaluation.accuracy == best_accuracy and tuned_self_evaluation.loss < best_loss):\n",
    "            best_model = tuned_model\n",
    "            best_accuracy = tuned_self_evaluation.accuracy\n",
    "            best_loss = tuned_self_evaluation.loss\n",
    "\n",
    "        print(f\"Seed {seed}: Accuracy = {tuned_self_evaluation.accuracy:.4f}, Loss = {tuned_self_evaluation.loss:.4f}\")\n",
    "        print(f\"\\nBest model so far: Accuracy = {best_accuracy:.4f}, Loss = {best_loss:.4f}\")\n",
    "print(f\"\\nBest model: Accuracy = {best_accuracy:.4f}, Loss = {best_loss:.4f}\")\n"
   ],
   "id": "77d6394fb9c99117",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the model\n",
    "tuned_self_evaluation = best_model.make_inspector().evaluation()\n",
    "print(f\"Accuracy: {tuned_self_evaluation.accuracy} Loss:{tuned_self_evaluation.loss}\")"
   ],
   "id": "e59a4a211b365156",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def prediction_to_kaggle_format(model, threshold=0.5):\n",
    "    proba_survive = model.predict(tf_test, verbose=0)[:,0]\n",
    "    return pd.DataFrame({\n",
    "        \"PassengerId\": test[\"PassengerId\"],\n",
    "        \"Survived\": (proba_survive >= threshold).astype(int)\n",
    "    })\n",
    "\n",
    "def make_submission(kaggle_predictions):\n",
    "    path=\"./submission.csv\"\n",
    "    kaggle_predictions.to_csv(path, index=False)\n",
    "    print(f\"Submission exported to {path}\")\n",
    "\n",
    "kaggle_predictions = prediction_to_kaggle_format(best_model)\n",
    "make_submission(kaggle_predictions)\n",
    "!head ./submission.csv"
   ],
   "id": "93b57d7565d8ec96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4b80d8d7fe375f48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7b118e98f9c53011",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
